{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HplGJuJR01Rl"
   },
   "outputs": [],
   "source": [
    "### USE ONLY THESE PACKAGES ###\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrfsqemms0rN"
   },
   "source": [
    "### Upload the lyrics_dataset.zip in the colab's folder first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MuSRKIVGyYR",
    "outputId": "639ce7bb-ca13-422a-b0d0-4b0adcd8eed4"
   },
   "outputs": [],
   "source": [
    "!unzip lyrics_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfJnnXc7O58m"
   },
   "source": [
    "### Do not change this seed number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcZM4dCTpfz7",
    "outputId": "347e8090-61cf-4d8f-f73a-b4557d2bcc61"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fUfhvwLvAMf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Use below code to train your model with all lyrics\n",
    "#lyrics = list()\n",
    "#for txt_file in os.listdir('./lyrics_dataset'):\n",
    "#    if txt_file[0] != '.':\n",
    "#        target_txt = os.path.join('./lyrics_dataset', txt_file)\n",
    "#        print(target_txt)\n",
    "#        f = open(target_txt, 'r', encoding='UTF-8')\n",
    "#        curr_lyrics = f.readlines()\n",
    "#        for i in range(len(curr_lyrics)):\n",
    "#            curr_lyrics[i] = curr_lyrics[i].lower()\n",
    "#        curr_lyrics = list(set(curr_lyrics))\n",
    "#        lyrics += curr_lyrics\n",
    "#print(len(lyrics))\n",
    "\n",
    "## Use below code to train your model with specific artist's lyrics\n",
    "target_txt = './lyrics_dataset/bruce-springsteen.txt'\n",
    "f = open(target_txt, 'r')\n",
    "lyrics = f.readlines()\n",
    "for i in range(len(lyrics)):\n",
    "    lyrics[i] = lyrics[i].lower()\n",
    "lyrics = list(set(lyrics))\n",
    "\n",
    "#\"lyrics\" is a list of strings.\n",
    "#for l in lyrics:\n",
    "#     print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # This module provides regular expression matching operations\n",
    "import collections #This module is used to count the frequency of vocabularies\n",
    "#both moduls are standard libraries in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGJNDVzh9Bny"
   },
   "outputs": [],
   "source": [
    "class lyricsDataset(Dataset): # 12 points\n",
    "    def __init__(self, lyrics, window_len):\n",
    "        self.lyrics = lyrics\n",
    "        self.window_len = window_len\n",
    "        # window len means, that, if i-th word is target\n",
    "        # you would consider i-window_len ~ i+window_len words as context words\n",
    "\n",
    "        ##--------------write below-------------## <--- do not erase this afterwards\n",
    "        self.trimmed_lyrics = list()    \n",
    "        self.vocabulary = list()\n",
    "        self.frequency = dict()\n",
    "        # your code in here\n",
    "        # eliminate every character except for alphabet, number, and space (\" \")\n",
    "        for i in range(len(lyrics)):\n",
    "            filtered_string = re.sub(\"[^0-9a-z]+\", \" \", lyrics[i])\n",
    "            string2list = filtered_string.split() \n",
    "            if len(string2list) != 0:\n",
    "                self.trimmed_lyrics.append(string2list)\n",
    "        \n",
    "        # merge all trimmed_lyrics to one large list, then get vocabulary \n",
    "        merged_list = list()\n",
    "        for i in self.trimmed_lyrics:\n",
    "            merged_list.extend(i)\n",
    "        \n",
    "        list_set = set(merged_list) \n",
    "        self.vocabulary = list(list_set)\n",
    "        \n",
    "        # count the frequency of each vocabulary\n",
    "        self.frequency = collections.Counter(merged_list)\n",
    "        ##--------------write above-------------## <--- do not erase this   \n",
    "        \n",
    "        ### MUST TO-DO 1. (+3) -> self.trimmed_lyrics\n",
    "        # for strings in self.lyrics, \n",
    "        # (+2) write a code to eliminate every character except for alphabet, number, and space (\" \")\n",
    "        ### for example, - + ? ! ' \" [ ] ( ) <- char like this should be excluded. \n",
    "        # (+1) after that, split each string with respect to the space.\n",
    "        ### If proprocessed string is \"hello hello hello\", \n",
    "        ### a list ['hello', 'hello', 'hello'] should be generated.\n",
    "        ### Then, put that list into the self.trimmed_lyrics.\n",
    "        ### self.trimmed_lyrics needs to be a LIST which has LIST as element.\n",
    "\n",
    "        ### MUST TO-DO 2. (+2) -> self.vocabulary\n",
    "        # (+2) Put all words(string) in self.trimmed_lyrics to the self.vocabulary. \n",
    "        ### In self.vocabulary, each word needs to be unique.\n",
    "        ### which means, this list (self.vocabulary) should not have duplicated elements. \n",
    "        ### self.vocabulary needs to be a LIST which contains unique words in self.trimmed_lyrics\n",
    "        ### If your code contains duplicated words, you would not get the point.\n",
    "        ### If there is a word that are neglected from self.trimmed_lyrics, you would not get the point.\n",
    "\n",
    "        ### MUST TO-DO 3. (+2) -> self.frequency\n",
    "        # (+2) In self.frequency, Record how many times each word in self.vocabulary \n",
    "        ###                              appears in self.trimmed_lyrics.\n",
    "        ### For example, if \"love\" appears 100 times in self.trimmed_lyrics,\n",
    "        ### it should be: self.frequency[\"love\"] = 100\n",
    "\n",
    "    def __len__(self):\n",
    "        # DO NOT TOUCH BELOW. JUST USE BELOW CODE FOR YOUR __len__ \n",
    "        return len(self.trimmed_lyrics)\n",
    "        # DO NOT TOUCH ABOVE. JUST USE ABOVE CODE FOR YOUR __len__ \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ### MUST TO-DO 4. (+5) --> sample\n",
    "        ##--------------write below-------------##\n",
    "        sample = dict()\n",
    "        sample['pairs'] = list()\n",
    "        # your code here\n",
    "        #generate random number as the index of target_word\n",
    "        target_idx = np.random.randint(0,len(self.trimmed_lyrics[idx]))\n",
    "        #print(idx)\n",
    "        #print(target_idx)\n",
    "        \n",
    "        #if only one word is in the selected trimmed_lyrics[idx], let context_word same as target_word\n",
    "        #since there are not so many scentences with only one word, doing so will not affect the training of the network \n",
    "        if len(self.trimmed_lyrics[idx]) == 1:\n",
    "            sample['pairs'].append((self.vocabulary.index(self.trimmed_lyrics[idx][target_idx]),self.vocabulary.index(self.trimmed_lyrics[idx][target_idx])))\n",
    "        \n",
    "        #otherwise, select the neightbouring word as contex_word according to the window_length\n",
    "        else:\n",
    "            for i in range(-self.window_len,self.window_len+1):\n",
    "                context_idx = target_idx + i \n",
    "                #check if context_word is within rhe range of selected trimmed_lyrics[idx]\n",
    "                if context_idx >= 0 and context_idx < len(self.trimmed_lyrics[idx]) and context_idx != target_idx:\n",
    "                    sample['pairs'].append((self.vocabulary.index(self.trimmed_lyrics[idx][target_idx]),self.vocabulary.index(self.trimmed_lyrics[idx][context_idx])))\n",
    "        \n",
    "        return sample\n",
    "        ##--------------write above-------------##\n",
    "        # Bring out one list, from self.trimmed_lyrics. (i.e., self.trimmed_lyrics[idx])\n",
    "        # Then, the list should contain the splited sentence in each lyric.\n",
    "        ### i.e., self.trimmed_lyrics[idx] would look like ['hey', 'nice', 'to', 'meet', 'you']\n",
    "        # (+1) randomly select the target word from self.trimmed_lyrics[idx]\n",
    "        # (+4) based on selected target word, and self.window_len,\n",
    "        #      generate a tuple of (target word index, context word index),\n",
    "        #      and add that tuple into sample['pairs']\n",
    "        ### if 'nice' is randomly chosen as the target element, and self.window_len = 1\n",
    "        ### if self.vocabulary[100] == 'hey', \n",
    "        ###    self.vocabulary[500] == 'nice',\n",
    "        ###    self.vocabulary[300] == 'to'\n",
    "        ### sample['pairs'] should be [(500, 300), (500, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzSkbpoFQnv3"
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH BELOW. JUST USE THESE LINES.\n",
    "### PENALTY (-5) CAN BE APPLIED IF YOUR CODE DOES NOT WORK FOR VARIOUS VALUES OF WINDOW_LEN\n",
    "dataset = lyricsDataset(lyrics, 2)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "### DO NOT TOUCH ABOVE. JUST USE THESE LINES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5WbDFRf9yzI"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module): # 15 points\n",
    "    def __init__(self, num_vocabs, embed_dim = 300): ## do not change 'embed_dim' value.\n",
    "        ### MUST TO-DO 5 : (+5)\n",
    "        ##--------------write below-------------##\n",
    "        # your code here\n",
    "        ##--------------write above-------------##\n",
    "        # Define a model which can map word's integer index value to word embedding.\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = torch.nn.Linear(in_features = self.num_vocabs, out_features = self.embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self, pairs):\n",
    "        '''\n",
    "        input : pairs - one tuple of (target word index, context word index)\n",
    "        '''\n",
    "        ### MUST TO DO 6 : (+10)\n",
    "        ##--------------write below-------------##\n",
    "        # your code here\n",
    "        ##--------------write above-------------##\n",
    "        # return the embedding of target word and context word.\n",
    "        # i.e., return target_embed, context_embed\n",
    "        \n",
    "        # create one_hot encoding as input to the network, return embedding of target_word and context_word \n",
    "        target_onehot = torch.nn.functional.one_hot(pairs[0], num_classes=self.num_vocabs).float()\n",
    "        context_onehot = torch.nn.functional.one_hot(pairs[1], num_classes=self.num_vocabs).float()\n",
    "        target_embed = self.linear(target_onehot)\n",
    "        context_embed = self.linear(context_onehot)\n",
    "        \n",
    "        return target_embed, context_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_djd4UufFd9"
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\n",
    "model = Word2Vec(len(dataset.vocabulary))\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "device = 'cuda'\n",
    "### DO NOT TOUCH ABOVE. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\n",
    "### USE THIS GIVEN OPTIMIZER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rp8hvPWjHxe"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, sample_1, sample_2): # 13 points\n",
    "    ### MUST TO DO 6 : (+12)\n",
    "    ##--------------write below-------------##\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #create label for positive samples of first target_word\n",
    "    fisrt_positive_label = torch.ones(len(sample_1['pairs'])).to(device)\n",
    "    #create label for negative samples of first target_word\n",
    "    fisrt_negative_label = torch.zeros(len(sample_2['pairs'])).to(device)\n",
    "    \n",
    "    #create label for positive samples of second target_word\n",
    "    second_positive_label = torch.ones(len(sample_2['pairs'])).to(device)\n",
    "    #create label for negative samples of second target_word\n",
    "    second_negative_label = torch.zeros(len(sample_1['pairs'])).to(device)\n",
    "    \n",
    "    \n",
    "    #tuples of first target word \n",
    "    first_positive_sample = torch.LongTensor(len(sample_1['pairs']),2).to(device)\n",
    "    first_negative_sample = torch.LongTensor(len(sample_2['pairs']),2).to(device)\n",
    "    \n",
    "    #create tuples of positive samples of first target_word (target_word: first_target_word, context_word: context_word of first target_word)\n",
    "    for i in range(len(sample_1['pairs'])):\n",
    "        first_positive_sample[i][0] = sample_1['pairs'][i][0]\n",
    "        first_positive_sample[i][1] = sample_1['pairs'][i][1]      \n",
    "    \n",
    "    #create tuples of negative samples of first target_word (target_word: first_target_word, context_word: context_word of second target_word)\n",
    "    for i in range(len(sample_2['pairs'])):\n",
    "        first_negative_sample[i][0] = sample_1['pairs'][0][0]\n",
    "        first_negative_sample[i][1] = sample_2['pairs'][i][1]\n",
    "    \n",
    "    #tuples of second target word \n",
    "    second_positive_sample = torch.LongTensor(len(sample_2['pairs']),2).to(device)\n",
    "    second_negative_sample = torch.LongTensor(len(sample_1['pairs']),2).to(device)\n",
    "        \n",
    "    #create tuples of positive samples of second target_word (target_word: second_target_word, context_word: context_word of second target_word)\n",
    "    for i in range(len(sample_2['pairs'])):\n",
    "        second_positive_sample[i][0] = sample_2['pairs'][i][0]\n",
    "        second_positive_sample[i][1] = sample_2['pairs'][i][1]      \n",
    "    \n",
    "    #create tuples of nagetive samples of second target_word (target_word: second_target_word, context_word: context_word of first target_word)\n",
    "    for i in range(len(sample_1['pairs'])):\n",
    "        second_negative_sample[i][0] = sample_2['pairs'][0][0]\n",
    "        second_negative_sample[i][1] = sample_1['pairs'][i][1]\n",
    "  \n",
    "    \n",
    "    # prediction for positve samples of first target_word\n",
    "    first_positive_prediction = torch.zeros(len(sample_1['pairs'])).to(device)\n",
    "    # prediction for negative samples of first target_word\n",
    "    first_negative_prediction = torch.zeros(len(sample_2['pairs'])).to(device)\n",
    "    \n",
    "    # prediction for positve samples of second target_word\n",
    "    second_positive_prediction = torch.zeros(len(sample_2['pairs'])).to(device)\n",
    "    # prediction for negative samples of second target_word\n",
    "    second_negative_prediction = torch.zeros(len(sample_1['pairs'])).to(device)\n",
    "\n",
    "    #prediction, using inner_product and sigmoid function \n",
    "    for i in range(len(sample_1['pairs'])):\n",
    "        target_embed, context_embed = model((first_positive_sample[i][0],first_positive_sample[i][1]))\n",
    "        inner_product = torch.dot(torch.flatten(target_embed),torch.flatten(context_embed))\n",
    "        first_positive_prediction[i] = torch.sigmoid(inner_product)\n",
    "    \n",
    "    for i in range(len(sample_2['pairs'])):\n",
    "        target_embed, context_embed = model((first_negative_sample[i][0],first_negative_sample[i][1]))\n",
    "        inner_product = torch.dot(torch.flatten(target_embed),torch.flatten(context_embed))\n",
    "        first_negative_prediction[i] = torch.sigmoid(inner_product)\n",
    "    \n",
    "    for i in range(len(sample_2['pairs'])):\n",
    "        target_embed, context_embed = model((second_positive_sample[i][0],second_positive_sample[i][1]))\n",
    "        inner_product = torch.dot(torch.flatten(target_embed),torch.flatten(context_embed))\n",
    "        second_positive_prediction[i] = torch.sigmoid(inner_product)\n",
    "        \n",
    "    for i in range(len(sample_1['pairs'])):\n",
    "        target_embed, context_embed = model((second_negative_sample[i][0],second_negative_sample[i][1]))\n",
    "        inner_product = torch.dot(torch.flatten(target_embed),torch.flatten(context_embed))\n",
    "        second_negative_prediction[i] = torch.sigmoid(inner_product)\n",
    "    \n",
    "    #calculate BCE loss for each sample_pairs \n",
    "    criterion = nn.BCELoss(reduction='sum') #need to calculate sigmoid \n",
    "    first_positive_loss = criterion(first_positive_prediction, fisrt_positive_label)\n",
    "    first_negative_loss = criterion(first_negative_prediction, fisrt_negative_label)\n",
    "    second_positive_loss = criterion(second_positive_prediction, second_positive_label)\n",
    "    second_negative_loss = criterion(second_negative_prediction, second_negative_label)\n",
    "    \n",
    "    #aaverage loss (total_loss averaged by number of samples) \n",
    "    total_loss = (first_positive_loss+first_negative_loss+second_positive_loss+second_negative_loss)/(2*(len(sample_1['pairs'])+len(sample_2['pairs'])))\n",
    "        \n",
    "    total_loss.backward()\n",
    "    # your code here\n",
    "    optimizer.step()\n",
    "    return total_loss\n",
    "    # return the current loss value\n",
    "    ##--------------write above-------------##\n",
    "    \n",
    "    ## sample_1 will contain positive (target_1, context_1) tuples\n",
    "    ## sample_2 will contain another positive (target_2, context_2) tuples\n",
    "    ## But for the negative sampling, we need negative (target, context) tuples.\n",
    "    ## NEGATIVE TUPLES can be generated by (target_1, context_2), (target_2, context_1)\n",
    "    ## (+5) Generate and give positive & negative tuples for model's input\n",
    "    ##### Then, you would get : target_embed, context_embed = model((target, context)).\n",
    "    ## (+4) calculate the distance between target_embed and context_embed by DOT PRODUCT\n",
    "    ## (+4) calculate the loss based on that distance, and optimize the model\n",
    "    ####    Label positive tuples as class '1', otherwise as class '0'\n",
    "    ####    You can also use sigmoid function rather than the softmax function.\n",
    "    ####    At the end, you must return the current loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB5jT1Pklc06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\n",
    "### PENALTY (-5) WILL BE GIVEN WHEN YOUR CODE RAISES AN ERROR DURING EPOCH.\n",
    "### YOUR TRAINING NEEDS TO BE PERFORMED WITH VARIOUS .TXT FILES.\n",
    "max_epoch = 5\n",
    "for epoch in range(max_epoch):\n",
    "    total_loss = 0.0\n",
    "    cnt = 0\n",
    "    for sample in tqdm(dataloader):\n",
    "        if cnt > 0:\n",
    "            curr_loss = train(model, optimizer, sample, prev_sample)\n",
    "            total_loss += curr_loss / len(dataloader)    \n",
    "        prev_sample = sample\n",
    "        cnt += 1\n",
    "        if cnt % 200 == 0:\n",
    "            print('[EPOCH {}] SAMPLED TRAIN LOSS : {}'.format(epoch, curr_loss))\n",
    "    print('[EPOCH {}] TOTAL LOSS : {}'.format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsf3XQMbuGFE"
   },
   "outputs": [],
   "source": [
    "### MUST TO DO 7 : (+10)\n",
    "##--------------write below-------------##\n",
    "# your code here\n",
    "for param in model.parameters():\n",
    "    embedding_tensor = param\n",
    "    print(1)\n",
    "embedding = embedding_tensor.cpu().detach().numpy()\n",
    "embedding = embedding.transpose()\n",
    "##--------------write above-------------##\n",
    "## (+10) bring your word embedding as a numpy array \"embedding\"\n",
    "#### \"embedding\" should be N by D array, \n",
    "#### where N is the number of vocabularies, and D is the dimension of the word embedding.\n",
    "#### embedding[i, :] should be word embedding of dataset.vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS8ukQiejMAI"
   },
   "outputs": [],
   "source": [
    "### JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\n",
    "reducer = PCA(n_components=2)\n",
    "# or try use\n",
    "# reducer = TSNE(n_components=2, verbose=1)\n",
    "reduce_results = reducer.fit_transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTMFi8ydvKF8"
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\n",
    "top_k = 50\n",
    "sort_idx = np.argsort(list(dataset.frequency.values()))[::-1]\n",
    "sort_idx = sort_idx[:top_k]\n",
    "frequent_vocabs = [list(dataset.frequency.keys())[si] for si in sort_idx]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, vocab in enumerate(dataset.vocabulary):\n",
    "    if vocab in frequent_vocabs:\n",
    "        plt.plot(reduce_results[idx, 0], reduce_results[idx, 1], '.')\n",
    "        plt.text(reduce_results[idx, 0], reduce_results[idx, 1], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFd4SQ5N9aRJ"
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\n",
    "min_dist = 10000000\n",
    "target_word = 'i'\n",
    "for idx, vocab in enumerate(dataset.vocabulary):\n",
    "    if vocab != target_word:\n",
    "        distance = np.linalg.norm(embedding[dataset.vocabulary.index(target_word)] - embedding[idx])\n",
    "        min_dist = min(distance, min_dist)\n",
    "        if min_dist == distance:\n",
    "            nearest_to_target = vocab\n",
    "print('\"{}\" is nearest to \"{}\"'.format(target_word, nearest_to_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[SKELETON] Assignment 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
